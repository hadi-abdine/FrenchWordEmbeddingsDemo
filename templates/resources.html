{% extends "base.html" %}
{% block title %} resources {% endblock %}

{% block script %}
<script src="{{ url_for('static', filename='JQuery.js') }}" type="text/javascript">
</script>
<script type="text/javascript">
  $(function() {
      var url = window.location.href;
      var activeTab = url.substring(url.indexOf("#") + 1);
      if (activeTab=="barthez"){
        // $(".tab-pane").removeClass("active");
        $("#"+activeTab).addClass("active");
        $('a[href="#'+ activeTab +'"]').tab('show');
      }
      if (activeTab=="bertweetfr"){
        // $(".tab-pane").removeClass("active");
        $("#"+activeTab).addClass("active");
        $('a[href="#'+ activeTab +'"]').tab('show');
      }
    });

</script>

{% endblock %}
{% block content %}


<div class="card mb-3 mx-auto" style="max-width: 80%; border-radius: 25px">
    <!-- <div class="card-header text-white bg-primary" style="border-top-right-radius: 25px;
     border-top-left-radius: 25px" >Vector Word Representation For The French Language</div> -->
    <div class="card-body">
        <h1 class="card-title text-green" style="color:Green;">French linguistic resources</h1>
        <!-- <h6 class="card-subtitle mb-2 text-muted">Card subtitle</h6> -->
        <p class="card-text"></p>
        <!-- <a href="#" class="card-link">Card link</a> -->
        <!-- <a href="#" class="card-link">Another link</a> -->
        <ul class="nav nav-tabs mb-3" id="pills-tab" role="tablist">
            <li class="nav-item">
                <a class="nav-link active" id="pills-embd-tab" data-toggle="pill" href="#pills-embd" role="tab"
                   aria-controls="pills-embd" aria-selected="true">Word2Vec</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" id="pills-barthez-tab" data-toggle="pill" href="#barthez" role="tab"
                   aria-controls="barthez" aria-selected="false">Seq2Seq: BARThez</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" id="pills-bertweetfr-tab" data-toggle="pill" href="#bertweetfr" role="tab"
                   aria-controls="bertweetfr" aria-selected="false">BERTweetFR</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" id="pills-corp-tab" data-toggle="pill" href="#pills-corp" role="tab"
                   aria-controls="pills-corp" aria-selected="false">N-grams</a>
            </li>
        </ul>

        <div class="tab-content" id="pills-tabContent">
            <div class="tab-pane fade show active" id="pills-embd" role="tabpanel" aria-labelledby="pills-embd-tab">


                <ul class="nav nav-tabs mb-3" id="pills-tab" role="tablist">
                    <li class="nav-item">
                        <a class="nav-link active" id="pills-d1-tab" data-toggle="pill" href="#pills-d1" role="tab"
                           aria-controls="pills-d1" aria-selected="true">fr_w2v_web_w5</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="pills-d2-tab" data-toggle="pill" href="#pills-d2" role="tab"
                           aria-controls="pills-d2" aria-selected="false">fr_w2v_web_w20</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="pills-f1-tab" data-toggle="pill" href="#pills-f1" role="tab"
                           aria-controls="pills-f1" aria-selected="false">fr_w2v_fl_w5</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="pills-f2-tab" data-toggle="pill" href="#pills-f2" role="tab"
                           aria-controls="pills-f2" aria-selected="false">fr_w2v_fl_w20</a>
                    </li>
                </ul>
                <div class="tab-content" id="pills-tabContent">
                    <div class="tab-pane fade show active in" id="pills-d1" role="tabpanel"
                         aria-labelledby="pills-d1-tab">
                        <p align="justify">French word vectors trained using word2vec CBOW with window size of 5 and
                            minimum word
                            frequency of 60. These vectors are trained on the 33GB deduplicated French raw text
                            collected from the web
                            and preprocessed in our group.<br>
                        </p>
                        <ul>
                            <!-- <li>.vec file : download (add link)</li> -->
                            <li>binary file:
                                <a href="{{ url_for('download_file', filename='fr_w2v_web_w5.bin') }}">
                                    download</a></li>
                        </ul>
                    </div>
                    <div class="tab-pane fade" id="pills-d2" role="tabpanel" aria-labelledby="pills-d2-tab">
                        <p align="justify">French word vectors trained using word2vec CBOW with window size of 20 and
                            minimum word
                            frequency of 5. These vectors are trained on the 33GB deduplicated French raw text collected
                            from the web
                            and preprocessed in our group. You can test these vectors in this web application.<br>
                        </p>
                        <ul>
                            <!-- <li>.vec file : download (add link)</li> -->
                            <li>binary file: <a href="{{ url_for('download_file', filename='fr_w2v_web_w20.bin') }}">
                                download</a></li>
                        </ul>
                    </div>
                    <div class="tab-pane fade" id="pills-f1" role="tabpanel" aria-labelledby="pills-f1-tab">
                        <p align="justify">French word vectors trained using word2vec CBOW with window size of 5 and
                            minimum word
                            frequency of 60. These vectors are trained on a 33GB shuffled portion of the French corpus
                            used to train
                            FlauBERT.<br>
                        </p>
                        <ul>
                            <!-- <li>.vec file : download (add link)</li> -->
                            <li>binary file: <a href="{{ url_for('download_file', filename='fr_w2v_fl_w5.bin') }}">
                                download</a></li>
                        </ul>
                    </div>
                    <div class="tab-pane fade" id="pills-f2" role="tabpanel" aria-labelledby="pills-f2-tab">
                        <p align="justify">French word vectors trained using word2vec CBOW with window size of 20 and
                            minimum word
                            frequency of 5. These vectors are trained on a 33GB shuffled portion of the French corpus
                            used to train
                            FlauBERT.<br>
                        </p>
                        <ul>
                            <!-- <li>.vec file : download (add link)</li> -->
                            <li>binary file: <a href="{{ url_for('download_file', filename='fr_w2v_fl_w20.bin') }}">
                                download</a></li>
                        </ul>
                    </div>

                </div>
                The resources are available for academic/non profit use. Interested parties please contact the group
                leader Prof
                M. Vazirgiannis mvazirg~lix.polytechnique.fr
            </div>
            <div class="tab-pane fade" id="barthez" role="tabpanel" aria-labelledby="pills-barthez-tab"
                 align="justify">
                BARThez is the first french sequence to sequence pretrained model. <br>
                BARThez is pretrained on 66GB of french raw text for roughly 60 hours on 128 Nvidia V100 GPUs using the
                CNRS
                <a href="http://www.idris.fr/annonces/annonce-jean-zay-eng.html">
                    Jean Zay supercomputer</a>.
                Our model is based on <a href="https://arxiv.org/abs/1910.13461">BART</a>. Unlike already existing
                BERT-based French language models such as CamemBERT and FlauBERT, BARThez is particularly well-suited
                for
                generative tasks, since not only its encoder but also its decoder is pretrained.
                In addition to BARThez that is pretrained from scratch, we continue the pretraining of a multilingual
                BART
                <a href="https://arxiv.org/abs/2001.08210">mBART25</a> which boosted its
                performance in both discriminative and generative tasks. We call the french adapted version mBARThez.
                Our models are competitive to CamemBERT and FlauBERT in discriminative tasks and outperform them in
                generative tasks such as abstractive summarization.<br>
                Paper: <a href="https://arxiv.org/abs/2010.12321">BARThez: a Skilled Pretrained French
                Sequence-to-Sequence
                Model</a><br>
                Github: <a href="https://github.com/moussaKam/BARThez">https://github.com/moussaKam/BARThez</a><br>
                Our models is now on HuggingFace: <a href="https://huggingface.co/moussaKam/barthez">BARThez</a>,&nbsp;
                <a
                        href="https://huggingface.co/moussaKam/mbarthez">mBARThez</a><br><br>
            </div>

            <div class="tab-pane fade" id="bertweetfr" role="tabpanel" aria-labelledby="pills-bertweetfr-tab"
                 align="justify">
                BBERTweetFR is the first pre-trained large scale language model adapted to French tweets. It is initialized
            with CamemBERT, the state-of-art general-domain language model for French based on the RoBERTa architecture.
            We perform domain-adaptive pre-training on 182M deduplicated tweets. The training runs for roughly 20 hours
            on 8 Nvidia V100 GPUs (32GB each) using the CNRS <a
                href="http://www.idris.fr/annonces/annonce-jean-zay-eng.html">
            Jean Zay supercomputer</a>. We evaluated BERTweetFR on three downstream Twitter analytic tasks including
            offensiveness classification, named entity recognition and unsupervised semantic shift detection, all
            yielding significant improvement compared to general-domain models.<br>
            Our model are now on HuggingFace : <a
                href="https://huggingface.co/Yanzhu/bertweetfr-base">BERTweetFR</a><br>
            You can also download below the fine-tuned ones for
            offensiveness classification and NER.<br><br>
                <ul>
                    <li>BERTweetFR fine-tuned on NER: <a href="{{ url_for('download_file', filename='ner_model.zip') }}">
                        download</a><br>
                    To use this model in transformers:<br>
                      <div class="card text-white bg-white" style="border-radius: 25px">
                        <div class="card-body">
                      <pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CamembertTokenizer, AutoModelForTokenClassification

bertweetfr_tokenizer = CamembertTokenizer.from_pretrained(<span class="hljs-string">'path/to/NER_model'</span>)

bertweet_model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">'path/to/NER_model'</span>)
</code></pre>
                        </div>
                      </div><br>
                    </li>
                    <li>BERTweetFR fine-tuned on offensiveness classification: <a
                            href="{{ url_for('download_file', filename='offensiveness_model.zip') }}">
                        download</a><br>
                    To use this model in transformers:<br>
                      <div class="card text-white bg-white" style="border-radius: 25px">
                        <div class="card-body">
                      <pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CamembertTokenizer, AutoModelForSequenceClassification

bertweetfr_tokenizer = CamembertTokenizer.from_pretrained(<span class="hljs-string">'path/to/offensiveness_classification_model'</span>)

bertweetfr_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">'path/to/offensiveness_classification_model'</span>)</code></pre>
                        </div>
                      </div><br></li>

                </ul>
            </div>

            <div class="tab-pane fade" id="pills-corp" role="tabpanel" aria-labelledby="pills-corp-tab">
                <ul>
                    <li>List of cleaned uni-grams: <a
                            href="{{ url_for('download_file', filename='cleaned_unigrams') }}">
                        download</a></li>
                    <li>List of cleaned bi-grams: <a href="{{ url_for('download_file', filename='cleaned_bigrams') }}">
                        download</a></li>
                    <li>List of cleaned tri-grams: <a
                            href="{{ url_for('download_file', filename='cleaned_trigrams') }}">
                        download</a></li>
                </ul>

            </div>
            <!-- <div class="tab-pane fade" id="pills-uni" role="tabpanel" aria-labelledby="pills-uni-tab">...</div>
            <div class="tab-pane fade" id="pills-bi" role="tabpanel" aria-labelledby="pills-bi-tab">...</div>
            <div class="tab-pane fade" id="pills-tri" role="tabpanel" aria-labelledby="pills-tri-tab">...</div> -->

            If you use these language resources, please cite the following papers:
            <div class="card text-white bg-dark" style="border-radius: 25px">
                <div class="card-body">
                    @article{eddine2020barthez,<br>
                    &nbsp; title={BARThez: a Skilled Pretrained French Sequence-to-Sequence Model},<br>
                    &nbsp; author={Eddine, Moussa Kamal and Tixier, Antoine J-P and Vazirgiannis, Michalis},<br>
                    &nbsp; journal={arXiv preprint arXiv:2010.12321},<br>
                    &nbsp; year={2020}<br>
                    }
                    <br><br>
                    @article{abdine2021evaluation,<br>
                    &nbsp; title={Evaluation Of Word Embeddings From Large-Scale French Web Content},<br>
                    &nbsp; author={Hadi Abdine and Christos Xypolopoulos and Moussa Kamal Eddine and Michalis
                    Vazirgiannis},<br>
                    &nbsp; year={2021},<br>
                    &nbsp; eprint={2105.01990},<br>
                    &nbsp; archivePrefix={arXiv},<br>
                    &nbsp; primaryClass={cs.CL}<br>
                    }
                    <br>

                </div>
            </div>
        </div>
    </div>
</div>


{% endblock %}